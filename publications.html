<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="æ™ºèƒ½æˆåƒç ”ç©¶å°ç»„ - ç§‘ç ”æˆæœ">
    <title>ç§‘ç ”æˆæœ - IÂ²Group</title>
    <link rel="stylesheet" href="css/style.css">
    <style>
        /* ========== é¡µé¢åŸºç¡€æ ·å¼ ========== */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --primary: #2563eb;
            --secondary: #7c3aed;
            --dark: #1e293b;
            --gray: #64748b;
            --light-bg: #f8fafc;
            --transition: all 0.3s ease;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            line-height: 1.6;
            color: var(--dark);
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 2rem;
        }

        /* ========== é¡µé¢æ ‡é¢˜åŒº ========== */
        .page-header {
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            color: white;
            padding: 5rem 0 3rem;
            text-align: center;
        }

        .page-header h1 {
            font-size: 3rem;
            margin-bottom: 1rem;
            font-weight: 800;
        }

        .page-header p {
            font-size: 1.2rem;
            opacity: 0.95;
            max-width: 700px;
            margin: 0 auto;
        }

        /* ========== è®ºæ–‡åˆ—è¡¨åŒºåŸŸ ========== */
        .publications-section {
            padding: 4rem 0;
            background: var(--light-bg);
        }

        .pub-list {
            display: grid;
            gap: 3rem;
        }

        /* ========== è®ºæ–‡å¡ç‰‡ ========== */
        .pub-card {
            background: white;
            border-radius: 16px;
            overflow: hidden;
            box-shadow: 0 4px 20px rgba(0,0,0,0.08);
            transition: var(--transition);
        }

        .pub-card:hover {
            transform: translateY(-8px);
            box-shadow: 0 8px 30px rgba(0,0,0,0.12);
        }

        /* æœ‰å›¾ç‰‡çš„è®ºæ–‡å¡ç‰‡ */
        .pub-card.with-image {
            display: grid;
            grid-template-columns: 400px 1fr;
        }

        .pub-image {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }

        /* è®ºæ–‡å†…å®¹åŒº */
        .pub-content {
            padding: 2rem;
        }

        .pub-badge {
            display: inline-block;
            padding: 0.4rem 1rem;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            color: white;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 600;
            margin-bottom: 1rem;
        }

        .pub-title {
            font-size: 1.5rem;
            font-weight: 700;
            color: var(--dark);
            margin-bottom: 1rem;
            line-height: 1.4;
        }

        .pub-authors {
            color: var(--gray);
            font-size: 0.95rem;
            margin-bottom: 1.5rem;
        }

        /* æ‘˜è¦ */
        .pub-abstract {
            background: #fff8e1;
            border-left: 4px solid #ffa726;
            padding: 1.2rem;
            margin-bottom: 1.5rem;
            border-radius: 8px;
        }

        .abstract-label {
            font-weight: 700;
            color: #e65100;
            margin-bottom: 0.6rem;
            font-size: 0.9rem;
        }

        .abstract-text {
            font-size: 0.9rem;
            line-height: 1.7;
            color: var(--dark);
        }

        .abstract-empty {
            color: var(--gray);
            font-style: italic;
        }

        /* å…ƒæ•°æ® */
        .pub-meta {
            display: grid;
            gap: 1rem;
            margin-bottom: 1.5rem;
            padding: 1.2rem;
            background: #f8fafc;
            border-radius: 8px;
        }

        .meta-row {
            display: flex;
            gap: 1rem;
            font-size: 0.9rem;
        }

        .meta-label {
            font-weight: 700;
            color: var(--primary);
            min-width: 100px;
        }

        .meta-value {
            color: var(--dark);
            flex: 1;
        }

        .tags {
            display: flex;
            gap: 0.5rem;
            flex-wrap: wrap;
        }

        .tag {
            padding: 0.3rem 0.8rem;
            background: white;
            border: 2px solid var(--primary);
            color: var(--primary);
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 600;
            transition: var(--transition);
        }

        .tag:hover {
            background: var(--primary);
            color: white;
        }

        /* æŸ¥çœ‹è®ºæ–‡æŒ‰é’® */
        .pub-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.8rem 1.5rem;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            color: white;
            text-decoration: none;
            border-radius: 25px;
            font-weight: 600;
            transition: var(--transition);
        }

        .pub-link:hover {
            transform: translateX(5px);
            box-shadow: 0 4px 15px rgba(37, 99, 235, 0.3);
        }

        /* ========== å“åº”å¼è®¾è®¡ ========== */
        @media (max-width: 968px) {
            .pub-card.with-image {
                grid-template-columns: 1fr;
            }

            .page-header h1 {
                font-size: 2.5rem;
            }

            .pub-title {
                font-size: 1.3rem;
            }
        }

        @media (max-width: 640px) {
            .page-header {
                padding: 3rem 0 2rem;
            }

            .page-header h1 {
                font-size: 2rem;
            }

            .pub-content {
                padding: 1.5rem;
            }

            .meta-row {
                flex-direction: column;
                gap: 0.5rem;
            }

            .meta-label {
                min-width: auto;
            }
        }
    </style>
</head>
<body>
    <!-- å¯¼èˆªæ  -->
    <nav class="navbar">
        <div class="container nav-container">
            <div class="logo">IÂ²Group</div>
            <ul class="nav-menu" id="navMenu">
                <li><a href="index.html" class="nav-link">Home</a></li>
                <li><a href="teams.html" class="nav-link">Team</a></li>
                <li><a href="publications.html" class="nav-link active">Publications</a></li>
                <li><a href="contact.html" class="nav-link">Contact</a></li>
            </ul>
            <button class="nav-toggle" id="navToggle">
                <span></span>
                <span></span>
                <span></span>
            </button>
        </div>
    </nav>

    <!-- é¡µé¢æ ‡é¢˜ -->
    <header class="page-header">
        <div class="container">
            <h1>ğŸ“š ç§‘ç ”æˆæœ</h1>
            <p>æ™ºèƒ½æˆåƒç ”ç©¶å°ç»„è‡´åŠ›äºå›¾åƒä¿¡å·å¤„ç†ã€å›¾åƒè´¨é‡è¯„ä¼°ã€é¢œè‰²æ’å¸¸æ€§ç­‰å‰æ²¿é¢†åŸŸç ”ç©¶</p>
        </div>
    </header>

    <!-- è®ºæ–‡åˆ—è¡¨ -->
    <section class="publications-section">
        <div class="container">
            <div class="pub-list">

                <!-- è®ºæ–‡1 - æœ‰å›¾ç‰‡ -->
                <article class="pub-card with-image">
                    <img src="images/è®ºæ–‡ä¸€.jpg" alt="è®ºæ–‡é…å›¾" class="pub-image">
                    <div class="pub-content">
                        <span class="pub-badge">TPAMI</span>
                        <h2 class="pub-title">Accelerated Self-Supervised Multi-Illumination Color Constancy With Hybrid Knowledge Distillation</h2>
                        <p class="pub-authors">Ziyu Feng; Bing Li; Congyan Lang; Zheming Xu; Haina Qin; Juan Wang</p>
                        
                        <div class="pub-abstract">
                            <div class="abstract-label">ğŸ“ Abstract</div>
                            <div class="abstract-text abstract-empty">æ— </div>
                        </div>

                        <div class="pub-meta">
                            <div class="meta-row">
                                <span class="meta-label">ğŸ·ï¸ Tags:</span>
                                <div class="meta-value">
                                    <div class="tags">
                                        <span class="tag">Color Constancy</span>
                                        <span class="tag">Knowledge Distillation</span>
                                        <span class="tag">Self-Supervised</span>
                                    </div>
                                </div>
                            </div>
                            <div class="meta-row">
                                <span class="meta-label">ğŸ“‚ Categories:</span>
                                <span class="meta-value">Computer Vision, Image Processing</span>
                            </div>
                            <div class="meta-row">
                                <span class="meta-label">ğŸ•’ Updated:</span>
                                <span class="meta-value">January 15, 2024</span>
                            </div>
                        </div>

                        <a href="https://ieeexplore.ieee.org/document/11051054" target="_blank" class="pub-link">
                            æŸ¥çœ‹è®ºæ–‡ â†’
                        </a>
                    </div>
                </article>

                <!-- è®ºæ–‡2 -->
                <article class="pub-card">
                    <div class="pub-content">
                        <span class="pub-badge">TPAMI</span>
                        <h2 class="pub-title">Ranking-based color constancy with limited training samples</h2>
                        <p class="pub-authors">Bing Li, Hanlin Qin, Weijia Xiong, Yinqiang Zheng, Shiming Feng, Weicai Hu, Stephen Maybank</p>
                        
                        <div class="pub-abstract">
                            <div class="abstract-label">ğŸ“ Abstract</div>
                            <div class="abstract-text">
                               Computational color constancy is an important component of Image Signal Processors (ISP) for white balancing in many imaging devices. Recently, deep convolutional neural networks (CNN) have been introduced for color constancy. They achieve prominent performance improvements comparing with those statistics or shallow learning-based methods. However, the need for a large number of training samples, a high computational cost and a huge model size make CNN-based methods unsuitable for deployment on low-resource ISPs for real-time applications. In order to overcome these limitations and to achieve comparable performance to CNN-based methods, an efficient method is defined for selecting the optimal simple statistics-based method (SM) for each image. To this end, we propose a novel ranking-based color constancy method (RCC) that formulates the selection of the optimal SM method as a label ranking problem. RCC designs a specific ranking loss function, and uses a low rank constraint to control the model complexity and a grouped sparse constraint for feature selection. Finally, we apply the RCC model to predict the order of the candidate SM methods for a test image, and then estimate its illumination using the predicted optimal SM method (or fusing the results estimated by the top $k$ SM methods). Comprehensive experiment results show that the proposed RCC outperforms nearly all the shallow learning-based methods and achieves comparable performance to (sometimes even better performance than) deep CNN-based methods with only 1/2000 of the model size and training time. RCC also shows good robustness to limited training samples and good generalization crossing cameras. Furthermore, to remove the dependence on the ground truth illumination, we extend RCC to obtain a novel ranking-based method without ground truth illumination (RCC_NO) that learns the ranking model using simple partial binary preference annotations provided by untrained annotators rather than experts. RCC_NO also achieves better performance than the SM methods and most shallow learning-based methods with low costs of sample collection and illumination measurement.Â 
                            </div>
                        </div>

                        <div class="pub-meta">
                            <div class="meta-row">
                                <span class="meta-label">ğŸ·ï¸ Tags:</span>
                                <div class="meta-value">
                                    <div class="tags">
                                        <span class="tag">Color Constancy</span>
                                        <span class="tag">Ranking-based Learning</span>
                                        <span class="tag">Limited Samples</span>
                                    </div>
                                </div>
                            </div>
                            <div class="meta-row">
                                <span class="meta-label">ğŸ“‚ Categories:</span>
                                <span class="meta-value">Computer Vision</span>
                            </div>
                            <div class="meta-row">
                                <span class="meta-label">ğŸ•’ Updated:</span>
                                <span class="meta-value">2023</span>
                            </div>
                        </div>

                        <a href="https://ieeexplore.ieee.org/abstract/document/9870176" target="_blank" class="pub-link">
                            æŸ¥çœ‹è®ºæ–‡ â†’
                        </a>
                    </div>
                </article>

                <!-- è®ºæ–‡3 -->
                <article class="pub-card">
                    <div class="pub-content">
                        <span class="pub-badge">TPAMI</span>
                        <h2 class="pub-title">Self-Prior Guided Pixel Adversarial Networks for Blind Image Inpainting</h2>
                        <p class="pub-authors">Juan Wang, Chunyu Yuan, Bing Li, Yan Deng, Weicai Hu, Stephen Maybank</p>
                        
                        <div class="pub-abstract">
                            <div class="abstract-label">ğŸ“ Abstract</div>
                            <div class="abstract-text">
                                Blind image inpainting involves two critical aspects, i.e., â€œwhere to inpaintâ€ and â€œhow to inpaintâ€. Knowing â€œwhere to inpaintâ€ can eliminate the interference arising from corrupted pixel values; a good â€œhow to inpaintâ€ strategy yields high-quality inpainted results robust to various corruptions. In existing methods, these two aspects usually lack explicit and separate consideration. This paper fully explores these two aspects and proposes a self-prior guided inpainting network (SIN). The self-priors are obtained by detecting semantic-discontinuous regions and by predicting global semantic structures of the input image. On the one hand, the self-priors are incorporated into the SIN, which enables the SIN to perceive valid context information from uncorrupted regions and to synthesize semantic-aware textures for corrupted regions. On the other hand, the self-priors are reformulated to provide a pixel-wise adversarial feedback and a high-level semantic structure feedback, which can promote the semantic continuity of inpainted images. Experimental results demonstrate that our method achieves state-of-the-art performance in metric scores and in visual quality. It has an advantage over many existing methods that assume â€œwhere to inpaintâ€ is known in advance. Extensive experiments on a series of related image restoration tasks validate the effectiveness of our method in obtaining high-quality inpainting.Â 
                            </div>
                        </div>

                        <div class="pub-meta">
                            <div class="meta-row">
                                <span class="meta-label">ğŸ·ï¸ Tags:</span>
                                <div class="meta-value">
                                    <div class="tags">
                                        <span class="tag">Image Inpainting</span>
                                        <span class="tag">GAN</span>
                                        <span class="tag">Blind Inpainting</span>
                                    </div>
                                </div>
                            </div>
                            <div class="meta-row">
                                <span class="meta-label">ğŸ“‚ Categories:</span>
                                <span class="meta-value">Computer Vision</span>
                            </div>
                            <div class="meta-row">
                                <span class="meta-label">ğŸ•’ Updated:</span>
                                <span class="meta-value">2023</span>
                            </div>
                        </div>

                        <a href="https://ieeexplore.ieee.org/abstract/document/10105766" target="_blank" class="pub-link">
                            æŸ¥çœ‹è®ºæ–‡ â†’
                        </a>
                    </div>
                </article>

                <!-- è®ºæ–‡4 -->
                <article class="pub-card">
                    <div class="pub-content">
                        <span class="pub-badge">IJCV</span>
                        <h2 class="pub-title">Hierarchical Curriculum Learning for No-reference Image Quality Assessment</h2>
                        <p class="pub-authors">Juan Wang, Zhong Chen, Chunyu Yuan, Bing Li, Wei Ma, Weicai Hu</p>
                        
                        <div class="pub-abstract">
                            <div class="abstract-label">ğŸ“ Abstract</div>
                            <div class="abstract-text">
                                Despite remarkable success has been achieved by convolutional neural networks (CNNs) in no-reference image quality assessment (NR-IQA), there still exist many challenges in improving the performance of IQA for authentically distorted images. An important factor is that the insufficient annotated data limits the training of high-capacity CNNs to accommodate diverse distortions, complicated semantic structures and high-variance quality scores of these images. To address this problem, this paper proposes a hierarchical curriculum learning (HCL) framework for NR-IQA. The main idea of the proposed framework is to leverage the external data to learn the prior knowledge about IQA widely and progressively. Specifically, as a closely-related task with NR-IQA, image restoration is used as the first curriculum to learn the image quality related knowledge (i.e., semantic and distortion information) on massive distorted-reference image pairs. Then multiple lightweight subnetworks are designed to learn human scoring rules on multiple available synthetic IQA datasets independently, and a cross-dataset quality assessment correlation (CQAC) module is proposed to fully explore the similarities and diversities of different scoring rules. Finally, the whole model is fine-tuned on the target authentic IQA dataset to fuse the learned knowledge and adapt to the target data distribution. Experimental results show that our model achieves state-of-the-art performance on multiple standard authentic IQA datasets. Moreover, the generalization of our model is fully validated by the cross-dataset evaluation and the gMAD competition. In addition, extensive analyses prove that the proposed HCL framework is effective in improving the performance of our model.Â 
                            </div>
                        </div>

                        <div class="pub-meta">
                            <div class="meta-row">
                                <span class="meta-label">ğŸ·ï¸ Tags:</span>
                                <div class="meta-value">
                                    <div class="tags">
                                        <span class="tag">NR-IQA</span>
                                        <span class="tag">Curriculum Learning</span>
                                        <span class="tag">Deep Learning</span>
                                    </div>
                                </div>
                            </div>
                            <div class="meta-row">
                                <span class="meta-label">ğŸ“‚ Categories:</span>
                                <span class="meta-value">Image Quality Assessment</span>
                            </div>
                            <div class="meta-row">
                                <span class="meta-label">ğŸ•’ Updated:</span>
                                <span class="meta-value">2023</span>
                            </div>
                        </div>

                        <a href="https://link.springer.com/article/10.1007/s11263-023-01926-7" target="_blank" class="pub-link">
                            æŸ¥çœ‹è®ºæ–‡ â†’
                        </a>
                    </div>
                </article>

                <!-- è®ºæ–‡5 -->
                <article class="pub-card">
                    <div class="pub-content">
                        <span class="pub-badge">CVPR 2023</span>
                        <h2 class="pub-title">Learning to Exploit the Sequence-Specific Prior Knowledge for Image Processing Pipelines Optimization</h2>
                        <p class="pub-authors">Hanlin Qin, Luyao Han, Weijia Xiong, Juan Wang, Wei Ma, Bing Li, Weicai Hu</p>
                        
                        <div class="pub-abstract">
                            <div class="abstract-label">ğŸ“ Abstract</div>
                            <div class="abstract-text">The hardware image signal processing (ISP) pipeline is the intermediate layer between the imaging sensor and the downstream application, processing the sensor signal into an RGB image. The ISP is less programmable and consists of a series of processing modules. Each processing module handles a subtask and contains a set of tunable hyperparameters. A large number of hyperparameters form a complex mapping with the ISP output. The industry typically relies on manual and time-consuming hyperparameter tuning by image experts, biased towards human perception. Recently, several automatic ISP hyperparameter optimization methods using downstream evaluation metrics come into sight. However, existing methods for ISP tuning treat the high-dimensional parameter space as a global space for optimization and prediction all at once without inducing the structure knowledge of ISP. To this end, we propose a sequential ISP hyperparameter prediction framework that utilizes the sequential relationship within ISP modules and the similarity among parameters to guide the model sequence process. We validate the proposed method on object detection, image segmentation, and image quality tasks.Â </div>
                        </div>

                        <div class="pub-meta">
                            <div class="meta-row">
                                <span class="meta-label">ğŸ·ï¸ Tags:</span>
                                <div class="meta-value">
                                    <div class="tags">
                                        <span class="tag">ISP</span>
                                        <span class="tag">Pipeline Optimization</span>
                                        <span class="tag">Prior Knowledge</span>
                                    </div>
                                </div>
                            </div>
                            <div class="meta-row">
                                <span class="meta-label">ğŸ“‚ Categories:</span>
                                <span class="meta-value">Image Processing</span>
                            </div>
                            <div class="meta-row">
                                <span class="meta-label">ğŸ•’ Updated:</span>
                                <span class="meta-value">2023</span>
                            </div>
                        </div>

                        <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Qin_Learning_To_Exploit_the_Sequence-Specific_Prior_Knowledge_for_Image_Processing_CVPR_2023_paper.html" target="_blank" class="pub-link">
                            æŸ¥çœ‹è®ºæ–‡ â†’
                        </a>
                    </div>
                </article>

                <!-- è®ºæ–‡6 -->
                <article class="pub-card">
                    <div class="pub-content">
                        <span class="pub-badge">ACM MM 2023</span>
                        <h2 class="pub-title">SMM: Self-Supervised Multi-Illumination Color Constancy Model with Multiple Pretext Tasks</h2>
                        <p class="pub-authors">Ziyu Feng, Zheming Xu, Hanlin Qin, Congyan Lang, Bing Li, Weijia Xiong</p>
                        
                        <div class="pub-abstract">
                            <div class="abstract-label">ğŸ“ Abstract</div>
                            <div class="abstract-text">Color constancy is an important ability of the human visual system to perceive constant colors across different illumination. In this paper, we study a more practical yet challenging task, removing color cast by multiple spatial-varying illumination. Previous methods are limited by the scale of the current multi-illumination datasets, which hinders them from learning more discriminative features. Instead, we first propose a self-supervised multi-illumination color constancy model that leverages multiple pretext tasks to fully explore lighting color contextual information and inherent color information without using any manual annotations. During the pre-training phase, we train multiple Transformer-based encoders by learning multiple pretext tasks: (i) the local color distortion recovery task, which is carefully designed to learn lighting color contextual representation, and (ii) the colorization task, which is utilized to acquire inherent knowledge. In the downstream color constancy task, we fine-tune the encoders and design a lightweight decoder to obtain better illumination distributions with fewer parameters. Our lightweight architecture outperforms the state-of-the-art methods on the multi-illuminant benchmark (LSMI) and got robust performance on the single illuminant benchmark (NUS-8). Additionally, extensive ablation studies and visualization results demonstrate the effectiveness of integrating lighting color contextual and inherent color information learning in a self-supervised manner.</div>
                        </div>

                        <div class="pub-meta">
                            <div class="meta-row">
                                <span class="meta-label">ğŸ·ï¸ Tags:</span>
                                <div class="meta-value">
                                    <div class="tags">
                                        <span class="tag">Color Constancy</span>
                                        <span class="tag">Self-Supervised</span>
                                        <span class="tag">Multi-Illumination</span>
                                    </div>
                                </div>
                            </div>
                            <div class="meta-row">
                                <span class="meta-label">ğŸ“‚ Categories:</span>
                                <span class="meta-value">Computer Vision</span>
                            </div>
                            <div class="meta-row">
                                <span class="meta-label">ğŸ•’ Updated:</span>
                                <span class="meta-value">2023</span>
                            </div>
                        </div>

                        <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3612031" target="_blank" class="pub-link">
                            æŸ¥çœ‹è®ºæ–‡ â†’
                        </a>
                    </div>
                </article>

                <!-- è®ºæ–‡7 -->
                <article class="pub-card">
                    <div class="pub-content">
                        <span class="pub-badge">ECCV 2022</span>
                        <h2 class="pub-title">Attention-aware Learning for Hyperparameters Prediction in Image Processing Pipelines</h2>
                        <p class="pub-authors">Hanlin Qin, Luyao Han, Juan Wang, Bing Li</p>
                        
                        <div class="pub-abstract">
                            <div class="abstract-label">ğŸ“ Abstract</div>
                            <div class="abstract-text ">Between the imaging sensor and the image applications, the hardware image signal processing (ISP) pipelines reconstruct an RGB image from the sensor signal and feed it into downstream tasks. The processing blocks in ISPs depend on a set of tunable hyperparameters that have a complex interaction with the output. Manual setting by image experts is the traditional way of hyperparameter tuning, which is time-consuming and biased towards human perception. Recently, ISP has been optimized by the feedback of the downstream tasks based on different optimization algorithms. Unfortunately, these methods should keep parameters fixed during the inference stage for arbitrary input without considering that each image should have specific parameters based on its feature. To this end, we propose an attention-aware learning method that integrates the parameter prediction network into ISP tuning and utilizes the multi-attention mechanism to generate the attentive mapping between the input RAW image and the parameter space. The proposed method integrates downstream tasks end-to-end, predicting specific parameters for each image. We validate the proposed method on object detection, image segmentation, and human viewing tasks.Â </div>
                        </div>

                        <div class="pub-meta">
                            <div class="meta-row">
                                <span class="meta-label">ğŸ·ï¸ Tags:</span>
                                <div class="meta-value">
                                    <div class="tags">
                                        <span class="tag">ISP</span>
                                        <span class="tag">Attention Mechanism</span>
                                        <span class="tag">Hyperparameter Prediction</span>
                                    </div>
                                </div>
                            </div>
                            <div class="meta-row">
                                <span class="meta-label">ğŸ“‚ Categories:</span>
                                <span class="meta-value">Image Processing</span>
                            </div>
                            <div class="meta-row">
                                <span class="meta-label">ğŸ•’ Updated:</span>
                                <span class="meta-value">2022</span>
                            </div>
                        </div>

                        <a href="https://link.springer.com/chapter/10.1007/978-3-031-19797-0_31" target="_blank" class="pub-link">
                            æŸ¥çœ‹è®ºæ–‡ â†’
                        </a>
                    </div>
                </article>

                <!-- è®ºæ–‡8 -->
                <!-- è®ºæ–‡ - Teacher-Guided Learning for Blind Image Quality Assessment -->
<article class="pub-card">
    <div class="pub-content">
        <span class="pub-badge">ACCV 2022</span>
        <h2 class="pub-title">Teacher-Guided Learning for Blind Image Quality Assessment</h2>
        <p class="pub-authors">Z. Chen, J. Wang, B. Li, C. Yuan, W. Xiong, R. Cheng, W. Hu</p>
        
        <div class="pub-abstract">
            <div class="abstract-label">ğŸ“ Abstract</div>
            <div class="abstract-text">
                The performance of deep learning models for blind image quality assessment (BIQA) suffers from annotated data insufficiency. However, image restoration, as a closely-related task with BIQA, can easily acquire training data without annotation. Moreover, both image semantic and distortion information are vital knowledge for the two tasks to predict and improve image quality. Inspired by these, this paper proposes a novel BIQA framework, which builds an image restoration model as a teacher network (TN) to learn the two aspects of knowledge and then guides the student network (SN) for BIQA. In TN, multi-branch convolutions are leveraged for performing adaptive restoration from diversely distorted images to strengthen the knowledge learning. Then the knowledge is transferred to the SN and progressively aggregated by computing long-distance responses to improve BIQA on small annotated data. Experimental results show that our method outperforms many state-of-the-arts on both synthetic and authentic datasets. Besides, the generalization, robustness and effectiveness of our method are fully validated.
            </div>
        </div>

        <div class="pub-meta">
            <div class="meta-row">
                <span class="meta-label">ğŸ·ï¸ Tags:</span>
                <div class="meta-value">
                    <div class="tags">
                        <span class="tag">Image Quality Assessment</span>
                        <span class="tag">Knowledge Distillation</span>
                        <span class="tag">Image Restoration</span>
                    </div>
                </div>
            </div>
            <div class="meta-row">
                <span class="meta-label">ğŸ“‚ Categories:</span>
                <span class="meta-value">Computer Vision, Image Quality Assessment</span>
            </div>
            <div class="meta-row">
                <span class="meta-label">ğŸ•’ Updated:</span>
                <span class="meta-value">2022</span>
            </div>
        </div>

        <a href="https://github.com/chencn2020/TeacherIQA" target="_blank" class="pub-link">
            æŸ¥çœ‹è®ºæ–‡ â†’
        </a>
    </div>
</article>

            <!-- è®ºæ–‡ - Reinforcement Learning-based Sequential Optimization for Image Signal Processing -->
<article class="pub-card">
    <div class="pub-content">
        <span class="pub-badge">AAAI 2024</span>
        <h2 class="pub-title">Reinforcement Learning-based Sequential Optimization for Image Signal Processing</h2>
        <p class="pub-authors">X. Sun, Z. Zhao, L. Wei, C. Long, M. Cai, L. Han, J. Wang, B. Li, Y. Guo</p>
        
        <div class="pub-abstract">
            <div class="abstract-label">ğŸ“ Abstract</div>
            <div class="abstract-text abstract-empty">æ— </div>
        </div>

        <div class="pub-meta">
            <div class="meta-row">
                <span class="meta-label">ğŸ·ï¸ Tags:</span>
                <div class="meta-value">
                    <div class="tags">
                        <span class="tag">Image Signal Processing</span>
                        <span class="tag">Reinforcement Learning</span>
                        <span class="tag">Sequential Optimization</span>
                    </div>
                </div>
            </div>
            <div class="meta-row">
                <span class="meta-label">ğŸ“‚ Categories:</span>
                <span class="meta-value">Computer Vision, Image Processing</span>
            </div>
            <div class="meta-row">
                <span class="meta-label">ğŸ•’ Updated:</span>
                <span class="meta-value">2024</span>
            </div>
        </div>

        <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28327" target="_blank" class="pub-link">
            æŸ¥çœ‹è®ºæ–‡ â†’
        </a>
    </div>
</article>


            </div>
        </div>
    </section>

    <!-- é¡µè„š -->
    <footer class="footer">
        <div class="container">
            <p>&copy; 2024 æ™ºèƒ½æˆåƒç ”ç©¶å°ç»„ (IÂ²Group). All Rights Reserved.</p>
            <p class="footer-subtitle">ä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿå…¨å›½é‡ç‚¹å®éªŒå®¤</p>
        </div>
    </footer>

    <script src="js/main.js"></script>
</body>
</html>
